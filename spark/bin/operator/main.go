package main

import (
	"context"
	"crypto/tls"
	"database/sql"
	"errors"
	"flag"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	entsql "entgo.io/ent/dialect/sql"
	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
	"golang.org/x/sync/errgroup"

	"github.com/XSAM/otelsql"
	"github.com/go-co-op/gocron/v2"
	grpcmiddleware "github.com/grpc-ecosystem/go-grpc-middleware"
	"github.com/improbable-eng/grpc-web/go/grpcweb"
	"github.com/jackc/pgx/v5/stdlib"
	_ "github.com/lib/pq"
	"github.com/lightsparkdev/spark/common"
	"github.com/lightsparkdev/spark/common/logging"
	"github.com/lightsparkdev/spark/so"
	"github.com/lightsparkdev/spark/so/authn"
	"github.com/lightsparkdev/spark/so/authninternal"
	"github.com/lightsparkdev/spark/so/authz"
	"github.com/lightsparkdev/spark/so/chain"
	"github.com/lightsparkdev/spark/so/db"
	"github.com/lightsparkdev/spark/so/ent"
	_ "github.com/lightsparkdev/spark/so/ent/runtime"
	sparkerrors "github.com/lightsparkdev/spark/so/errors"

	sparkgrpc "github.com/lightsparkdev/spark/so/grpc"
	"github.com/lightsparkdev/spark/so/helper"
	"github.com/lightsparkdev/spark/so/knobs"
	"github.com/lightsparkdev/spark/so/middleware"
	events "github.com/lightsparkdev/spark/so/stream"
	"github.com/lightsparkdev/spark/so/task"
	_ "github.com/mattn/go-sqlite3"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc"
	"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	otelprom "go.opentelemetry.io/otel/exporters/prometheus"
	"go.opentelemetry.io/otel/propagation"
	"go.opentelemetry.io/otel/sdk/metric"
	semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
	"go.opentelemetry.io/otel/trace/noop"
	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
	"google.golang.org/grpc/keepalive"
)

type args struct {
	LogLevel                   string
	LogJSON                    bool
	LogRequestStats            bool
	ConfigFilePath             string
	Index                      uint64
	IdentityPrivateKeyFilePath string
	OperatorsFilePath          string
	Threshold                  uint64
	SignerAddress              string
	Port                       uint64
	DatabasePath               string
	RunningLocally             bool
	ChallengeTimeout           time.Duration
	SessionDuration            time.Duration
	AuthzEnforced              bool
	DisableDKG                 bool
	DisableChainwatcher        bool
	SupportedNetworks          string
	AWS                        bool
	ServerCertPath             string
	ServerKeyPath              string
	RunDirectory               string
	RateLimiterEnabled         bool
	RateLimiterMemcachedAddrs  string
	RateLimiterWindow          time.Duration
	RateLimiterMaxRequests     int
	RateLimiterMethods         string
	EntDebug                   bool
}

func (a *args) SupportedNetworksList() []common.Network {
	var networks []common.Network
	if strings.Contains(a.SupportedNetworks, "mainnet") || a.SupportedNetworks == "" {
		networks = append(networks, common.Mainnet)
	}
	if strings.Contains(a.SupportedNetworks, "testnet") || a.SupportedNetworks == "" {
		networks = append(networks, common.Testnet)
	}
	if strings.Contains(a.SupportedNetworks, "regtest") || a.SupportedNetworks == "" {
		networks = append(networks, common.Regtest)
	}
	if strings.Contains(a.SupportedNetworks, "signet") || a.SupportedNetworks == "" {
		networks = append(networks, common.Signet)
	}
	return networks
}

func loadArgs() (*args, error) {
	args := &args{}

	// Define flags
	flag.StringVar(&args.LogLevel, "log-level", "debug", "Logging level: debug|info|warn|error")
	flag.BoolVar(&args.LogJSON, "log-json", false, "Output logs in JSON format")
	flag.BoolVar(&args.LogRequestStats, "log-request-stats", false, "Log request stats (requires log-json)")
	flag.StringVar(&args.ConfigFilePath, "config", "so_config.yaml", "Path to config file")
	flag.Uint64Var(&args.Index, "index", 0, "Index value")
	flag.StringVar(&args.IdentityPrivateKeyFilePath, "key", "", "Identity private key")
	flag.StringVar(&args.OperatorsFilePath, "operators", "", "Path to operators file")
	flag.Uint64Var(&args.Threshold, "threshold", 0, "Threshold value")
	flag.StringVar(&args.SignerAddress, "signer", "", "Signer address")
	flag.Uint64Var(&args.Port, "port", 0, "Port value")
	flag.StringVar(&args.DatabasePath, "database", "", "Path to database file")
	flag.BoolVar(&args.RunningLocally, "local", false, "Running locally")
	flag.DurationVar(&args.ChallengeTimeout, "challenge-timeout", time.Minute, "Challenge timeout")
	flag.DurationVar(&args.SessionDuration, "session-duration", time.Minute*15, "Session duration")
	flag.BoolVar(&args.AuthzEnforced, "authz-enforced", true, "Enforce authorization checks")
	flag.BoolVar(&args.DisableDKG, "disable-dkg", false, "Disable DKG")
	flag.BoolVar(&args.DisableChainwatcher, "disable-chainwatcher", false, "Disable Chainwatcher")
	flag.StringVar(&args.SupportedNetworks, "supported-networks", "", "Supported networks")
	flag.BoolVar(&args.AWS, "aws", false, "Use AWS RDS")
	flag.StringVar(&args.ServerCertPath, "server-cert", "", "Path to server certificate")
	flag.StringVar(&args.ServerKeyPath, "server-key", "", "Path to server key")
	flag.StringVar(&args.RunDirectory, "run-dir", "", "Run directory for resolving relative paths")
	flag.BoolVar(&args.RateLimiterEnabled, "rate-limiter-enabled", false, "Enable rate limiting")
	flag.StringVar(&args.RateLimiterMemcachedAddrs, "rate-limiter-memcached-addrs", "", "Comma-separated list of Memcached addresses")
	flag.DurationVar(&args.RateLimiterWindow, "rate-limiter-window", 60*time.Second, "Rate limiter time window")
	flag.IntVar(&args.RateLimiterMaxRequests, "rate-limiter-max-requests", 100, "Maximum requests allowed in the time window")
	flag.StringVar(&args.RateLimiterMethods, "rate-limiter-methods", "", "Comma-separated list of methods to rate limit")
	flag.BoolVar(&args.EntDebug, "ent-debug", false, "Log all the SQL queries")

	// Parse flags
	flag.Parse()

	if args.IdentityPrivateKeyFilePath == "" {
		return nil, errors.New("identity private key file path is required")
	}

	if args.OperatorsFilePath == "" {
		return nil, errors.New("operators file is required")
	}

	if args.SignerAddress == "" {
		return nil, errors.New("signer address is required")
	}

	if args.Port == 0 {
		return nil, errors.New("port is required")
	}

	return args, nil
}

func createRateLimiter(config *so.Config, opts ...middleware.RateLimiterOption) (*middleware.RateLimiter, error) {
	if !config.RateLimiter.Enabled {
		return nil, nil
	}

	return middleware.NewRateLimiter(config, opts...)
}

type BufferedBody struct {
	BodyReader io.ReadCloser
	Body       []byte
	Position   int
}

func (body *BufferedBody) Read(p []byte) (n int, err error) {
	err = nil
	if body.Body == nil {
		body.Body, err = io.ReadAll(body.BodyReader)
	}

	n = copy(p, body.Body[body.Position:])
	body.Position += n
	if err == nil && body.Position == len(body.Body) {
		err = io.EOF
	}

	return n, err
}

func (body *BufferedBody) Close() error {
	return body.BodyReader.Close()
}

func NewBufferedBody(bodyReader io.ReadCloser) *BufferedBody {
	return &BufferedBody{bodyReader, nil, 0}
}

func main() {
	args, err := loadArgs()
	if err != nil {
		fmt.Fprintf(os.Stderr, "Failed to load args: %v\n", err)
		os.Exit(1)
	}

	logConfig := zap.NewProductionConfig()
	logLevel, err := zap.ParseAtomicLevel(args.LogLevel)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Failed to parse log level: %v\n", err)
		os.Exit(1)
	}

	logConfig.Level = logLevel

	if args.LogJSON {
		logConfig.Encoding = "json"
		logConfig.EncoderConfig = zap.NewProductionEncoderConfig()
	} else {
		logConfig.Encoding = "console"
		logConfig.EncoderConfig = zap.NewDevelopmentEncoderConfig()
	}

	// Various settings to make logs more similar to slog (both so they're backwards compatible with
	// downstream ingestion and just generally similar).
	logConfig.EncoderConfig.TimeKey = "time"
	logConfig.EncoderConfig.CallerKey = zapcore.OmitKey
	logConfig.EncoderConfig.EncodeTime = zapcore.RFC3339NanoTimeEncoder
	logConfig.EncoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder

	// Disable sampling to ensure all logs are captured.
	logConfig.Sampling = nil

	logger, err := logConfig.Build()
	if err != nil {
		fmt.Fprintf(os.Stderr, "Failed to build logger: %v\n", err)
		os.Exit(1)
	}
	defer logger.Sync() //nolint:errcheck

	logger = logger.WithOptions(zap.WrapCore(func(core zapcore.Core) zapcore.Core {
		return &logging.SourceCore{Core: core}
	}))

	sigCtx, done := signal.NotifyContext(context.Background(), syscall.SIGINT, syscall.SIGTERM)
	defer done()

	errGrp, errCtx := errgroup.WithContext(sigCtx)
	errCtx = logging.Inject(errCtx, logger)

	config, err := so.NewConfig(
		errCtx,
		args.ConfigFilePath,
		args.Index,
		args.IdentityPrivateKeyFilePath,
		args.OperatorsFilePath, // TODO: Refactor this into the yaml config
		args.Threshold,
		args.SignerAddress,
		args.DatabasePath,
		args.AWS,
		args.AuthzEnforced,
		args.SupportedNetworksList(),
		args.ServerCertPath,
		args.ServerKeyPath,
		args.RunDirectory,
		so.RateLimiterConfig{
			Enabled:     args.RateLimiterEnabled,
			Window:      args.RateLimiterWindow,
			MaxRequests: args.RateLimiterMaxRequests,
			Methods:     strings.Split(args.RateLimiterMethods, ","),
		},
	)
	if err != nil {
		logger.Fatal("Failed to create config", zap.Error(err))
	}

	// OBSERVABILITY
	promExporter, err := otelprom.New()
	if err != nil {
		logger.Fatal("Failed to create prometheus exporter", zap.Error(err))
	}
	meterProvider := metric.NewMeterProvider(metric.WithReader(promExporter))
	otel.SetMeterProvider(meterProvider)
	otel.SetTextMapPropagator(propagation.TraceContext{})

	if config.Tracing.Enabled {
		shutdown, err := common.ConfigureTracing(errCtx, config.Tracing)
		if err != nil {
			logger.Fatal("Failed to configure tracing", zap.Error(err))
		}
		defer func() {
			shutdownCtx, shutdownRelease := context.WithTimeout(context.Background(), 10*time.Second)
			defer shutdownRelease()

			logger.Info("Shutting down tracer provider")
			if err := shutdown(shutdownCtx); err != nil {
				logger.Error("Error shutting down tracer provider", zap.Error(err))
			} else {
				logger.Info("Tracer provider shut down")
			}
		}()
	}

	var valuesProvider knobs.KnobsValuesProvider
	if config.Knobs.IsEnabled() {
		if valuesProvider, err = knobs.NewKnobsK8ValuesProvider(errCtx, config.Knobs.Namespace); err != nil {
			// Knobs has failed to fetch the config, so the controllers will rely on the default values.
			logger.Error("Failed to create K8 knobs", zap.Error(err))
		}
	}

	// Knobs service is always defined, no need to check for nil.
	// If the provider is nil, the knobs service will use the default values.
	knobsService := knobs.New(valuesProvider)

	dbDriver := config.DatabaseDriver()
	connector, err := so.NewDBConnector(errCtx, config, knobsService)
	if err != nil {
		logger.Fatal("Failed to create db connector", zap.Error(err))
	}
	defer connector.Close()

	dbEvents, err := db.NewDBEvents(errCtx, connector, logger.With(zap.String("component", "dbevents")))
	if err != nil {
		logger.Fatal("Failed to create db events", zap.Error(err))
	}

	if config.Database.DBEventsEnabled != nil && *config.Database.DBEventsEnabled {
		errGrp.Go(func() error {
			return dbEvents.Start()
		})
	}

	for _, op := range config.SigningOperatorMap {
		op.SetTimeoutProvider(knobs.NewKnobsTimeoutProvider(knobsService, config.GRPC.ClientTimeout))
	}

	config.FrostGRPCConnectionFactory.SetTimeoutProvider(
		knobs.NewKnobsTimeoutProvider(knobsService, config.GRPC.ClientTimeout))

	var sqlDb entsql.ExecQuerier
	if dbDriver == "postgres" {
		sqlDb = stdlib.OpenDBFromPool(connector.Pool())
	} else {
		sqlDb = otelsql.OpenDB(connector, otelsql.WithSpanOptions(so.OtelSQLSpanOptions))
	}

	dialectDriver := entsql.NewDriver(dbDriver, entsql.Conn{ExecQuerier: sqlDb})

	var dbClient *ent.Client
	if args.EntDebug {
		dbClient = ent.NewClient(ent.Driver(dialectDriver), ent.Debug())
	} else {
		dbClient = ent.NewClient(ent.Driver(dialectDriver))
	}

	dbClient.Intercept(ent.DatabaseStatsInterceptor(10 * time.Second))
	defer dbClient.Close()

	if dbDriver == "sqlite3" {
		sqliteDb, _ := sql.Open("sqlite3", config.DatabasePath)
		if _, err := sqliteDb.ExecContext(errCtx, "PRAGMA journal_mode=WAL;"); err != nil {
			logger.Fatal("Failed to set journal_mode", zap.Error(err))
		}
		if _, err := sqliteDb.ExecContext(errCtx, "PRAGMA busy_timeout=5000;"); err != nil {
			logger.Fatal("Failed to set busy_timeout", zap.Error(err))
		}
		sqliteDb.Close()
	}

	frostConnection, err := config.NewFrostGRPCConnection()
	if err != nil {
		logger.Fatal("Failed to create frost client", zap.Error(err))
	}

	if !args.DisableChainwatcher {
		// Chain watchers
		for network, bitcoindConfig := range config.BitcoindConfigs {
			network := network
			bitcoindConfig := bitcoindConfig
			errGrp.Go(func() error {
				chainCtx, chainCancel := context.WithCancel(errCtx)
				defer chainCancel()

				chainLogger := logger.With(zap.String("component", "chainwatcher"), zap.String("network", network))
				chainCtx = logging.Inject(chainCtx, chainLogger)

				err := chain.WatchChain(
					chainCtx,
					config,
					dbClient,
					bitcoindConfig,
				)
				if err != nil {
					logger.Error("Error in chain watcher", zap.Error(err))
					return err
				}

				if errCtx.Err() == nil {
					// This technically isn't an error, but raise it as one because our chain watcher should never
					// stop unless we explicitly tell it to when shutting down!
					return fmt.Errorf("chain watcher for %s stopped unexpectedly", network)
				}

				return nil
			})
		}
	}

	if !args.DisableDKG {
		// Scheduled tasks setup
		cronCtx, cronCancel := context.WithCancel(errCtx)
		defer cronCancel()

		taskLogger := logger.With(zap.String("component", "cron"))
		cronCtx = logging.Inject(cronCtx, taskLogger)

		taskLogger.Info("Starting scheduler")
		taskMonitor, err := task.NewMonitor()
		if err != nil {
			taskLogger.Fatal("Failed to create task monitor", zap.Error(err))
		}
		scheduler, err := gocron.NewScheduler(
			gocron.WithGlobalJobOptions(
				gocron.WithContext(cronCtx),
				gocron.WithSingletonMode(gocron.LimitModeReschedule),
			),
			gocron.WithLogger(task.NewZapLoggerAdapter(taskLogger)),
			gocron.WithMonitorStatus(taskMonitor),
		)
		if err != nil {
			logger.Fatal("Failed to create scheduler", zap.Error(err))
		}
		for _, scheduled := range task.AllScheduledTasks() {
			// Don't run the task if the task specifies it should not be run in
			// test environments and RunningLocally is set (eg. we are in a test environment)
			if (!args.RunningLocally || scheduled.RunInTestEnv) && !scheduled.Disabled {
				err := scheduled.Schedule(scheduler, config, dbClient, knobsService)
				if err != nil {
					logger.Fatal("Failed to create job", zap.Error(err))
				}
			}
		}
		scheduler.Start()
		defer scheduler.Shutdown() //nolint:errcheck

		// Run startup tasks
		startupCtx, startupCancel := context.WithCancel(errCtx)
		defer startupCancel()

		errGrp.Go(func() error {
			// TODO(mhr): Do this properly, have a waitgroup in `RunStartupTasks` that waits until all tasks
			// are done before returning.
			startupCtx = logging.Inject(startupCtx, logger.With(zap.String("component", "startup")))

			return task.RunStartupTasks(startupCtx, config, dbClient, args.RunningLocally, knobsService)
		})
	}

	sessionTokenCreatorVerifier, err := authninternal.NewSessionTokenCreatorVerifier(config.IdentityPrivateKey, nil)
	if err != nil {
		logger.Fatal("Failed to create token verifier", zap.Error(err))
	}

	var rateLimiter *middleware.RateLimiter
	logger.Sugar().Infof(
		"Rate limiter config: enabled %t, window %s, max requests %d, methods %+q",
		config.RateLimiter.Enabled,
		config.RateLimiter.Window,
		config.RateLimiter.MaxRequests,
		config.RateLimiter.Methods,
	)
	if config.RateLimiter.Enabled {
		var err error
		rateLimiter, err = createRateLimiter(config, middleware.WithKnobs(knobsService))
		if err != nil {
			logger.Fatal("Failed to create rate limiter", zap.Error(err))
		}
	}

	clientInfoProvider := sparkgrpc.NewGRPCClientInfoProvider(config.XffClientIpPosition)
	var tableLogger *logging.TableLogger
	if args.LogRequestStats && args.LogJSON {
		tableLogger = logging.NewTableLogger(clientInfoProvider)
	}

	serverOpts := []grpc.ServerOption{
		grpc.StatsHandler(otelgrpc.NewServerHandler()),
	}

	// Establish base values from config, then allow runtime knobs to override
	// grpcConnTimeout, grpcKeepaliveTime and grpcKeepaliveTimeout are set when
	// the server is created and cannot be changed at runtime.
	grpcConnTimeout := knobsService.GetDuration(knobs.KnobGrpcServerConnectionTimeout, config.GRPC.ServerConnectionTimeout)
	grpcKeepaliveTime := knobsService.GetDuration(knobs.KnobGrpcServerKeepaliveTime, config.GRPC.ServerKeepaliveTime)
	grpcKeepaliveTimeout := knobsService.GetDuration(knobs.KnobGrpcServerKeepaliveTimeout, config.GRPC.ServerKeepaliveTimeout)

	// This uses SetDeadline in net.Conn to set the timeout for the connection
	// establishment, after which the connection is closed with error
	// `DeadlineExceeded`.
	if grpcConnTimeout > 0 {
		serverOpts = append(serverOpts, grpc.ConnectionTimeout(grpcConnTimeout))
	}

	// Keepalive detects dead connections and closes them.
	// Time is the interval between keepalive pings.
	// Timeout is the interval between keepalive pings after which the connection is closed.
	serverOpts = append(serverOpts, grpc.KeepaliveParams(keepalive.ServerParameters{
		Time:    grpcKeepaliveTime,
		Timeout: grpcKeepaliveTimeout,
	}))

	concurrencyGuard := sparkgrpc.NewConcurrencyGuard(knobsService, config.GRPC.ServerConcurrencyLimit)

	var eventsRouter *events.EventRouter
	if config.Database.DBEventsEnabled != nil && *config.Database.DBEventsEnabled {
		eventsRouter = events.NewEventRouter(dbClient, dbEvents, logger.With(zap.String("component", "events_router")))
	}

	// Add Interceptors aka gRPC middleware
	//
	// Interceptors wrap RPC handlers so we can apply crossâ€‘cutting concerns in one place
	// and in a defined order. We install separate chains for unary (request/response)
	// and streaming RPCs.
	serverOpts = append(serverOpts,
		grpc.UnaryInterceptor(grpcmiddleware.ChainUnaryServer(
			sparkerrors.ErrorInterceptor(config.ReturnDetailedErrors),
			sparkgrpc.LogInterceptor(logger.With(zap.String("component", "grpc")), tableLogger),
			sparkgrpc.SparkTokenMetricsInterceptor(),
			// Inject knobs into context for unary requests
			func() grpc.UnaryServerInterceptor {
				return func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (any, error) {
					ctx = knobs.InjectKnobsService(ctx, knobsService)
					return handler(ctx, req)
				}
			}(),
			sparkgrpc.ConcurrencyInterceptor(concurrencyGuard),
			sparkgrpc.TimeoutInterceptor(knobsService, config.GRPC.ServerUnaryHandlerTimeout),
			sparkgrpc.PanicRecoveryInterceptor(config.ReturnDetailedPanicErrors),
			func() grpc.UnaryServerInterceptor {
				if rateLimiter != nil {
					return rateLimiter.UnaryServerInterceptor()
				}
				return func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (any, error) {
					return handler(ctx, req)
				}
			}(),
			sparkgrpc.DatabaseSessionMiddleware(
				db.NewDefaultSessionFactory(dbClient),
				config.Database.NewTxTimeout,
			),
			helper.SigningCommitmentInterceptor(config.SigningOperatorMap, knobsService),
			authn.NewInterceptor(sessionTokenCreatorVerifier).AuthnInterceptor,
			authz.NewAuthzInterceptor(authz.NewAuthzConfig(
				authz.WithMode(config.ServiceAuthz.Mode),
				authz.WithAllowedIPs(config.ServiceAuthz.IPAllowlist),
				authz.WithProtectedServices(GetProtectedServices()),
				authz.WithXffClientIpPosition(config.XffClientIpPosition),
			)).UnaryServerInterceptor,
			sparkgrpc.ValidationInterceptor(),
		)),
		grpc.StreamInterceptor(grpcmiddleware.ChainStreamServer(
			sparkerrors.ErrorStreamingInterceptor(),
			sparkgrpc.StreamLogInterceptor(logger.With(zap.String("component", "grpc"))),
			sparkgrpc.PanicRecoveryStreamInterceptor(),
			authn.NewInterceptor(sessionTokenCreatorVerifier).StreamAuthnInterceptor,
			authz.NewAuthzInterceptor(authz.NewAuthzConfig(
				authz.WithMode(config.ServiceAuthz.Mode),
				authz.WithAllowedIPs(config.ServiceAuthz.IPAllowlist),
				authz.WithProtectedServices(GetProtectedServices()),
				authz.WithXffClientIpPosition(config.XffClientIpPosition),
			)).StreamServerInterceptor,
			sparkgrpc.StreamValidationInterceptor(),
		)),
	)

	cert, err := tls.LoadX509KeyPair(args.ServerCertPath, args.ServerKeyPath)
	if err != nil {
		logger.Fatal("Failed to load server certificate", zap.Error(err))
	}

	tlsConfig := tls.Config{
		Certificates: []tls.Certificate{cert},
		ClientAuth:   tls.NoClientCert,
		MinVersion:   tls.VersionTLS12,
	}

	creds := credentials.NewTLS(&tlsConfig)
	serverOpts = append(serverOpts, grpc.Creds(creds))
	grpcServer := grpc.NewServer(serverOpts...)

	err = RegisterGrpcServers(
		grpcServer,
		args,
		config,
		logger,
		dbClient,
		frostConnection,
		sessionTokenCreatorVerifier,
		eventsRouter,
	)
	if err != nil {
		logger.Fatal("Failed to register all gRPC servers", zap.Error(err))
	}

	// Web compatibility layer
	wrappedGrpc := grpcweb.WrapServer(grpcServer,
		grpcweb.WithOriginFunc(func(_ string) bool {
			return true
		}),
		grpcweb.WithCorsForRegisteredEndpointsOnly(false),
	)

	mux := http.NewServeMux()
	mux.Handle("/-/ready", http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) {
		w.WriteHeader(http.StatusOK)
	}))
	mux.Handle("/metrics", promhttp.Handler())
	mux.Handle("/",
		otelhttp.NewHandler(
			http.HandlerFunc(
				func(w http.ResponseWriter, r *http.Request) {
					// The gRPC server doesn't read the request body until EOF before processing
					// the request. This can result in the HTTP server receiving a DATA(END_FRAME)
					// frame after sending the response, which elicits a RST_STREAM(STREAM_CLOSED)
					// frame. ALB and nginx then respond to the client with RST_STREAM(INTERNAL_ERROR)
					// which causes the request to fail. The workaround is to buffer the entire
					// request body before passing to the gRPC server.
					r.Body = NewBufferedBody(r.Body)

					if strings.ToLower(r.Header.Get("Content-Type")) == "application/grpc" {
						grpcServer.ServeHTTP(w, r)
						return
					}
					wrappedGrpc.ServeHTTP(w, r)
				},
			),
			"server",
			otelhttp.WithTracerProvider(noop.TracerProvider{}), // Disable tracing, let gRPC server handle it.
			otelhttp.WithMetricAttributesFn(func(r *http.Request) []attribute.KeyValue {
				return []attribute.KeyValue{
					// Technically we shouldn't be using the path here because of cardinality, but since we know
					// this is just routing to the gRPC server, we can assume the path is reasonable.
					attribute.String(string(semconv.HTTPRouteKey), r.URL.Path),
				}
			}),
		),
	)

	server := &http.Server{
		Addr:      fmt.Sprintf(":%d", args.Port),
		Handler:   mux,
		TLSConfig: &tlsConfig,
	}

	errGrp.Go(func() error {
		if err := server.ListenAndServeTLS("", ""); !errors.Is(err, http.ErrServerClosed) {
			logger.Error("HTTP server failed", zap.Error(err))
			return err
		}

		return nil
	})

	// Now we wait... for something to fail.
	<-errCtx.Done()

	if sigCtx.Err() != nil {
		logger.Info("Received shutdown signal, shutting down gracefully...")
	} else {
		logger.Error("Shutting down due to error...")
	}

	logger.Info("Stopping gRPC server...")
	grpcServer.GracefulStop()
	logger.Info("gRPC server stopped")

	shutdownCtx, shutdownRelease := context.WithTimeout(context.Background(), 10*time.Second)
	defer shutdownRelease()

	logger.Info("Stopping HTTP server...")
	if err := server.Shutdown(shutdownCtx); err != nil {
		logger.Error("HTTP server failed to shutdown gracefully", zap.Error(err))
	} else {
		logger.Info("HTTP server stopped")
	}

	if err := errGrp.Wait(); err != nil {
		logger.Error("Shutdown due to error", zap.Error(err))
	}
}
